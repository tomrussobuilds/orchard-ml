# ==============================================================================
# FILE: recipes/optuna_cifar100_pretrained.yaml
# Pretrained Architecture Search on CIFAR-100 (32x32) with Optuna
# ==============================================================================
# Usage: orchard run recipes/optuna_cifar100_pretrained.yaml
# Estimated Time: ~3-4h GPU (50 trials × 15 epochs, RTX 5070)
# Memory Usage: ~2GB VRAM per trial
#
# PURPOSE:
#   Compare pretrained weight sources on CIFAR-100. All models use
#   pretrained: true — the timm model name encodes the specific weights.
#   CIFAR shares semantic categories with ImageNet (cat, dog, bird, etc.)
#   so results should be interpreted with the overlap caveat in mind.
#   See ARCHITECTURE.md "Extended Weight Sources via timm" for details.
#
# ARCHITECTURE POOL:
#   - resnet_18                               : Built-in, ImageNet-1k (torchvision)
#                                               Small stem with 7→3 weight interpolation
#   - timm/resnet18.a1_in1k                   : timm ImageNet-1k, A1 training recipe
#                                               (stronger augmentation, longer schedule)
#   - timm/resnet18.fb_ssl_yfcc100m_ft_in1k   : Self-supervised pretraining on YFCC100M
#                                               (100M Flickr images), then IN1k fine-tune
#                                               → Lower ImageNet label bias
#   - timm/resnet18.fb_swsl_ig1b_ft_in1k      : Semi-weakly supervised on Instagram 1B
#                                               (1B images, hashtag labels), then IN1k fine-tune
#                                               → Broadest visual vocabulary
#
# NOTE: The built-in resnet_18 uses Orchard's stem adaptation (3x3 stride-1)
#   for 32×32. timm/ models use their own stem — at 32×32 the standard 7x7
#   stride-2 stem produces very small feature maps, which may hurt performance.
#   This is an intentional comparison point.

dataset:
  name: "cifar100"
  resolution: 32
  force_rgb: true
  use_weighted_sampler: false

architecture:
  name: "resnet_18"
  pretrained: true                 # Use pretrained weights for all pool models
  dropout: 0.3

training:
  seed: 42
  batch_size: 128
  learning_rate: 0.01              # Lower LR for fine-tuning pretrained models
  weight_decay: 5e-4
  momentum: 0.9
  min_lr: 1e-6
  mixup_alpha: 0.0
  label_smoothing: 0.0
  epochs: 100
  patience: 20
  grad_clip: 1.0
  mixup_epochs: 100             # Capped to optuna.epochs by config_builder
  monitor_metric: "auc"
  scheduler_type: "cosine"
  scheduler_patience: 5
  scheduler_factor: 0.1
  step_size: 20
  use_amp: true
  use_tta: false
  criterion_type: "cross_entropy"
  weighted_loss: false
  focal_gamma: 2.0

augmentation:
  hflip: 0.5
  rotation_angle: 5
  jitter_val: 0.05
  min_scale: 0.95
  tta_translate: 0.5
  tta_scale: 1.02
  tta_blur_sigma: 0.1
  tta_blur_kernel_size: 3

hardware:
  device: "auto"
  reproducible: false

telemetry:
  output_dir: ./outputs
  log_level: "INFO"
  log_interval: 50

evaluation:
  n_samples: 16
  fig_dpi: 200
  cmap_confusion: Blues
  plot_style: seaborn-v0_8-muted
  grid_cols: 4
  fig_size_predictions: [12, 8]
  report_format: xlsx
  save_confusion_matrix: true
  save_predictions_grid: true

optuna:
  study_name: "cifar100_pretrained_comparison"
  n_trials: 50
  epochs: 15
  timeout: null
  metric_name: "auc"
  direction: "maximize"
  enable_early_stopping: true
  early_stopping_threshold: 0.9999
  early_stopping_patience: 2
  sampler_type: "tpe"
  search_space_preset: "full"
  enable_model_search: true
  # Compare pretrained weight sources: torchvision vs timm A1 vs SSL vs SWSL
  model_pool:
    - "resnet_18"                             # Built-in ImageNet-1k (stem-adapted)
    - "timm/resnet18.a1_in1k"                 # timm ImageNet-1k (A1 recipe)
    - "timm/resnet18.fb_ssl_yfcc100m_ft_in1k" # SSL on YFCC100M → IN1k fine-tune
    - "timm/resnet18.fb_swsl_ig1b_ft_in1k"    # SWSL on Instagram 1B → IN1k fine-tune
  enable_pruning: true
  pruner_type: "median"
  pruning_warmup_epochs: 5
  storage_type: "sqlite"
  storage_path: null
  n_jobs: 1
  load_if_exists: true
  show_progress_bar: false
  save_plots: true
  save_best_config: true

  search_space_overrides:
      learning_rate:
          low: 1.0e-05
          high: 0.005               # Narrower range for fine-tuning
          log: true
      weight_decay:
          low: 1.0e-06
          high: 0.001
          log: true
      momentum:
          low: 0.85
          high: 0.95
          log: false
      min_lr:
          low: 1.0e-07
          high: 1.0e-05
          log: true
      mixup_alpha:
          low: 0.0
          high: 0.4
          log: false
      label_smoothing:
          low: 0.0
          high: 0.2
          log: false
      dropout:
          low: 0.1
          high: 0.5
          log: false
      scheduler_patience:
          low: 3
          high: 10
      rotation_angle:
          low: 0
          high: 15
      jitter_val:
          low: 0.0
          high: 0.15
          log: false
      min_scale:
          low: 0.9
          high: 1.0
          log: false
      batch_size_low_res:
      - 16
      - 32
      - 48
      - 64
      batch_size_high_res:
      - 8
      - 12
      - 16

tracking:
  enabled: true
  experiment_name: "orchard-ml"

export:
  format: onnx
  opset_version: 18
  validate_export: true
