# üîÆ VisionForge: Type-Safe Deep Learning Framework

---

<!-- Badges Section -->
<p align="center">
  <!-- CI/CD & Quality -->
  <a href="https://github.com/tomrussobuilds/visionforge/actions/workflows/ci.yml">
    <img src="https://github.com/tomrussobuilds/visionforge/actions/workflows/ci.yml/badge.svg" alt="CI/CD Pipeline">
  </a>
  <a href="https://codecov.io/gh/tomrussobuilds/visionforge">
    <img src="https://codecov.io/gh/tomrussobuilds/visionforge/branch/main/graph/badge.svg" alt="Coverage">
  </a>
  <a href="https://docs.pytest.org/">
    <img src="https://img.shields.io/badge/tested%20with-pytest-blue?logo=pytest&logoColor=white" alt="Tested with pytest">
  </a>
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/license-MIT-green" alt="License">
  </a>
</p>

<p align="center">
  <!-- Core Technologies -->
  <img src="https://img.shields.io/badge/python-3.10%2B-blue?logo=python&logoColor=white" alt="Python">
  <a href="https://pytorch.org/">
    <img src="https://img.shields.io/badge/PyTorch-2.0%2B-orange?logo=pytorch&logoColor=white" alt="PyTorch">
  </a>
  <a href="https://docs.pydantic.dev/">
    <img src="https://img.shields.io/badge/Pydantic-v2-e92063?logo=pydantic&logoColor=white" alt="Pydantic">
  </a>
  <a href="https://optuna.org/">
    <img src="https://img.shields.io/badge/Optuna-3.0%2B-00ADD8?logo=optuna&logoColor=white" alt="Optuna">
  </a>
</p>

<p align="center">
  <!-- Code Quality & Status -->
  <a href="https://github.com/psf/black">
    <img src="https://img.shields.io/badge/code%20style-black-000000.svg" alt="Black">
  </a>
  <a href="https://pycqa.github.io/isort/">
    <img src="https://img.shields.io/badge/imports-isort-1674b1?logo=python&logoColor=white" alt="isort">
  </a>
  <a href="https://flake8.pycqa.org/">
    <img src="https://img.shields.io/badge/linting-flake8-brightgreen?logo=python&logoColor=white" alt="Flake8">
  </a>
  <img src="https://img.shields.io/badge/tests-800%2B-success" alt="Tests">
  <img src="https://img.shields.io/badge/coverage-%E2%86%92100%25-brightgreen" alt="Coverage Goal">
  <img src="https://img.shields.io/badge/Architecture-Decoupled-blueviolet" alt="Architecture">
  <img src="https://img.shields.io/badge/status-Active-success" alt="Status">
  <a href="https://github.com/tomrussobuilds/visionforge/issues">
    <img src="https://img.shields.io/github/issues/tomrussobuilds/visionforge" alt="GitHub Issues">
  </a>
</p>

---

## üìå Table of Contents

- [üéØ Overview](#-overview)
- [‚ö° Hardware Requirements](#-hardware-requirements)
- [üöÄ Quick Start](#-quick-start)
- [‚ú® Core Features](#-core-features)
- [üèó System Architecture](#-system-architecture)
- [üìä Experiment Management](#-experiment-management)
- [üß© Dependency Graph](#-dependency-graph)
- [üî¨ Technical Deep Dive](#-technical-deep-dive)
- [üìÅ Project Structure](#-project-structure)
- [üíª Usage Patterns](#-usage-patterns)
- [üéØ Hyperparameter Optimization](#-hyperparameter-optimization)
- [‚úÖ Environment Verification](#-environment-verification)
- [üê≥ Containerized Deployment](#-containerized-deployment)
- [üìä Configuration Reference](#-configuration-reference)
- [üîÑ Extending to New Datasets](#-extending-to-new-datasets)
- [üß™ Testing & Quality Assurance](#-testing--quality-assurance)
- [üìö Citation](#-citation)
- [üó∫ Development Roadmap](#-development-roadmap)

---

## üéØ Overview

**VisionForge** is a research-grade PyTorch training framework engineered for reproducible, scalable computer vision experiments. Originally designed for medical imaging (MedMNIST v2), it has evolved into a domain-agnostic platform supporting multi-resolution architectures (28√ó28 to 224√ó224+), automated hyperparameter optimization, and cluster-safe execution.

**Key Differentiators:**
- **Type-Safe Configuration Engine**: Pydantic V2-based declarative manifests eliminate runtime errors
- **Zero-Conflict Execution**: Kernel-level file locking (`fcntl`) prevents concurrent runs from corrupting shared resources
- **Intelligent Hyperparameter Search**: Optuna integration with TPE sampling and Median Pruning
- **Hardware-Agnostic**: Auto-detection and optimization for CPU/CUDA/MPS backends
- **Audit-Grade Traceability**: BLAKE2b-hashed run directories with full YAML snapshots

**Supported Architectures:**

| Resolution | Architectures | Parameters | Use Case |
|-----------|--------------|-----------|----------|
| **28√ó28** | ResNet-18-Adapted | ~11M | Transfer learning baseline |
| **28√ó28** | MiniCNN | ~50K | Fast prototyping, ablation studies |
| **224√ó224** | EfficientNet-B0 | ~5.3M | Efficient compound scaling |
| **224√ó224** | ViT-Tiny | ~5.7M | Patch-based attention, multiple weight variants |

---

## ‚ö° Hardware Requirements

### CPU Training (28√ó28 Only)
- **Supported Resolution**: 28√ó28 **only**
- **Time**: ~2.5 hours (ResNet-18-Adapted, 60 epochs, 16 cores)
- **Time**: ~30 minutes (MiniCNN, 60 epochs, 16 cores)
- **Architectures**: ResNet-18-Adapted, MiniCNN
- **Use Case**: Development, testing, limited hardware environments

### GPU Training (All Resolutions)
- **28√ó28 Resolution**: 
  - MiniCNN: ~2-3 minutes (60 epochs)
  - ResNet-18-Adapted: ~5 minutes (60 epochs)
- **224√ó224 Resolution**: 
  - EfficientNet-B0: ~30 minutes per trial (15 epochs)
  - ViT-Tiny: ~25-35 minutes per trial (30 epochs)
- **VRAM**: 8GB recommended for 224√ó224 resolution
- **Architectures**: All (ResNet-18-Adapted, MiniCNN, EfficientNet-B0, ViT-Tiny)

> [!WARNING]
> **224√ó224 training on CPU is not recommended** - it would take 10+ hours per trial. High-resolution training requires GPU acceleration. Only 28√ó28 resolution has been tested and validated for CPU training.

**Representative Benchmarks** (RTX 5070 Laptop GPU):

| Task | Architecture | Resolution | Device | Time | Notes |
|------|-------------|-----------|--------|------|-------|
| **Smoke Test** | MiniCNN | 28√ó28 | CPU/GPU | <30s | 1-epoch sanity check |
| **Quick Training** | MiniCNN | 28√ó28 | GPU | ~2-3 min | 60 epochs |
| **Quick Training** | MiniCNN | 28√ó28 | CPU (16 cores) | ~30 min | 60 epochs, CPU-validated |
| **Transfer Learning** | ResNet-18-Adapted | 28√ó28 | GPU | ~5 min | 60 epochs |
| **Transfer Learning** | ResNet-18-Adapted | 28√ó28 | CPU (16 cores) | ~2.5h | 60 epochs, CPU-validated |
| **High-Res Training** | EfficientNet-B0 | 224√ó224 | GPU | ~30 min | 15 epochs, **GPU required** |
| **High-Res Training** | ViT-Tiny | 224√ó224 | GPU | ~25-35 min | 30 epochs, **GPU required** |
| **Optimization Study** | EfficientNet-B0 | 224√ó224 | GPU | ~2h | 4 trials (early stop at AUC‚â•0.9999) |
| **Optimization Study** | Various | 224√ó224 | GPU | ~1.5-5h | 20 trials, highly variable |

>[!Note]
>**Timing Variance**: Optimization times are highly dependent on early stopping criteria, pruning configuration, and dataset complexity:
>- **Early Stopping**: Studies may finish in 1-3 hours if performance thresholds are met quickly (e.g., AUC ‚â• 0.9999 after 4 trials)
>- **Full Exploration**: Without early stopping, 20 trials can extend to 5+ hours
>- **Pruning Impact**: Median pruning can save 30-50% of total time by terminating underperforming trials

---

## üöÄ Quick Start

### Step 1: Environment Setup
```bash
# Clone and setup environment
git clone https://github.com/tomrussobuilds/visionforge.git
cd visionforge
pip install -r requirements.txt
```

### Step 2: Smoke Test (30 seconds - GPU/CPU)
```bash
# Verify installation with 1-epoch sanity check
# Downloads BloodMNIST 28√ó28 by default (or specify --dataset/--resolution)
python -m tests.smoke_test
```

### Step 3: Choose Your Workflow

#### üî¨ **Workflow A: Fast Prototyping** (28√ó28, CPU-friendly)
```bash
# Quick baseline (~2-3 min GPU, ~30 min CPU)
python main.py --config recipes/config_mini_cnn.yaml

# Transfer learning baseline (~5 min GPU, ~2.5h CPU)
python main.py --config recipes/config_resnet_18_adapted.yaml
```

#### üéØ **Workflow B: Hyperparameter Optimization ‚Üí Best Config** (Recommended)
```bash
# 1. Run optimization study (GPU recommended)
python optimize.py --config recipes/optuna_mini_cnn.yaml              # 28√ó28 (~1-2h)
python optimize.py --config recipes/optuna_efficientnet_b0.yaml       # 224√ó224 (~1.5-5h)

# 2. View optimization results (interactive HTML plots)
firefox outputs/*/figures/param_importances.html

# 3. Train with optimized hyperparameters (60 epochs, full validation)
python main.py --config outputs/*/reports/best_config.yaml
```

#### üèÜ **Workflow C: State-of-the-Art** (224√ó224, GPU required)
```bash
# EfficientNet-B0 (~30 min per trial, GPU)
python main.py --config recipes/config_efficientnet_b0.yaml

# Vision Transformer (~25-35 min per trial, GPU)
python main.py --config recipes/config_vit_tiny.yaml
```

### Step 4: Explore Results
```bash
# All artifacts in timestamped directories
ls outputs/YYYYMMDD_dataset_model_hash/
‚îú‚îÄ‚îÄ figures/          # Confusion matrices, training curves, sample predictions
‚îú‚îÄ‚îÄ reports/          # Excel summaries, study reports, best configs
‚îú‚îÄ‚îÄ checkpoints/      # Model weights (.pth)
‚îî‚îÄ‚îÄ database/         # Optuna SQLite studies (optimization only)
```

---

## ‚ú® Core Features

### üîí Enterprise-Grade Execution Safety

**Tiered Configuration Engine (SSOT)**  
Built on Pydantic V2, the configuration system acts as a **Single Source of Truth**, transforming raw inputs (CLI/YAML) into an immutable, type-safe execution blueprint:

- **Late-Binding Metadata Injection**: Dataset specifications (normalization constants, class mappings) are resolved from a centralized registry at instantiation time
- **Cross-Domain Validation**: Post-construction logic guards prevent unstable states (e.g., enforcing RGB input for pretrained weights, validating AMP compatibility)
- **Path Portability**: Automatic serialization converts absolute paths to environment-agnostic anchors for cross-platform reproducibility

**Infrastructure Guard Layer**  
An independent `InfrastructureManager` bridges declarative configs with physical hardware:

- **Mutual Exclusion via `flock`**: Kernel-level advisory locking ensures only one training instance per workspace (prevents VRAM race conditions)
- **Process Sanitization**: `psutil` wrapper identifies and terminates ghost Python processes
- **HPC-Aware Safety**: Auto-detects cluster schedulers (SLURM/PBS/LSF) and suspends aggressive process cleanup to preserve multi-user stability

**Deterministic Run Isolation**  
Every execution generates a unique workspace using:
```
outputs/YYYYMMDD_DS_MODEL_HASH6/
```
Where `HASH6` is a BLAKE2b cryptographic digest (3-byte, deterministic) computed from the training configuration. Even minor hyperparameter variations produce isolated directories, preventing resource overlap and ensuring auditability.

### üî¨ Reproducibility Architecture

**Dual-Layer Reproducibility Strategy:**
1. **Standard Mode**: Global seeding (Seed 42) with performance-optimized algorithms
2. **Strict Mode**: Bit-perfect reproducibility via:
   - `torch.use_deterministic_algorithms(True)`
   - `worker_init_fn` for multi-process RNG synchronization
   - Auto-scaling to `num_workers=0` when determinism is critical

**Data Integrity Validation:**
- MD5 checksum verification for dataset downloads
- `validate_npz_keys` structural integrity checks before memory allocation

### ‚ö° Performance Optimization

**Hybrid RAM Management:**
- **Small Datasets** (<50K samples): Full RAM caching for maximum throughput
- **Large Datasets** (>100K samples): Indexed slicing to prevent OOM errors

**Dynamic Path Anchoring:**
- "Search-up" logic locates project root via markers (`.git`, `README.md`)
- Ensures absolute path stability regardless of invocation directory

**Graceful Logger Reconfiguration:**
- Initial logs route to `STDOUT` for immediate feedback
- Hot-swap to timestamped file handler post-initialization without trace loss

### üéØ Intelligent Hyperparameter Search

**Optuna Integration Features:**
- **TPE Sampling**: Tree-structured Parzen Estimator for efficient search space exploration
- **Median Pruning**: Early stopping of underperforming trials (30-50% time savings)
- **Persistent Studies**: SQLite storage enables resume-from-checkpoint
- **Type-Safe Constraints**: All search spaces respect Pydantic validation bounds
- **Auto-Visualization**: Parameter importance plots, optimization history, parallel coordinates

---

## üèó System Architecture

The framework implements **Separation of Concerns (SoC)** with five core layers:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      RootOrchestrator                           ‚îÇ
‚îÇ              (Lifecycle Manager & Context)                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Responsibilities:                                              ‚îÇ
‚îÇ  ‚Ä¢ Phase 1-7 initialization sequence                            ‚îÇ
‚îÇ  ‚Ä¢ Resource acquisition & cleanup (Context Manager)             ‚îÇ
‚îÇ  ‚Ä¢ Device resolution & caching                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                         ‚îÇ
             ‚îÇ uses                    ‚îÇ uses
             ‚îÇ                         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ‚îÇ     ‚îÇ                        ‚îÇ
    ‚îÇ  Config Engine    ‚îÇ     ‚îÇ  InfrastructureManager ‚îÇ
    ‚îÇ  (Pydantic V2)    ‚îÇ     ‚îÇ  (flock/psutil)        ‚îÇ
    ‚îÇ                   ‚îÇ     ‚îÇ                        ‚îÇ
    ‚îÇ  ‚Ä¢ Type safety    ‚îÇ     ‚îÇ  ‚Ä¢ Process cleanup     ‚îÇ
    ‚îÇ  ‚Ä¢ Validation     ‚îÇ     ‚îÇ  ‚Ä¢ Kernel locks        ‚îÇ
    ‚îÇ  ‚Ä¢ Metadata       ‚îÇ     ‚îÇ  ‚Ä¢ HPC detection       ‚îÇ
    ‚îÇ    injection      ‚îÇ     ‚îÇ                        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚îÇ provides config to
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                                                        ‚îÇ
    ‚îÇ              Execution Pipeline                        ‚îÇ
    ‚îÇ                                                        ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
    ‚îÇ  ‚îÇ   Data   ‚îÇ  ‚îÇ  Model   ‚îÇ  ‚îÇ Trainer  ‚îÇ              ‚îÇ
    ‚îÇ  ‚îÇ Handler  ‚îÇ‚Üí ‚îÇ Factory  ‚îÇ‚Üí ‚îÇ  Engine  ‚îÇ              ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
    ‚îÇ                                   ‚îÇ                    ‚îÇ
    ‚îÇ                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
    ‚îÇ                             ‚îÇ Evaluation ‚îÇ             ‚îÇ
    ‚îÇ                             ‚îÇ  Pipeline  ‚îÇ             ‚îÇ
    ‚îÇ                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
    ‚îÇ                                                        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚îÇ alternative path
                                 ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ  Optimization Engine  ‚îÇ
                        ‚îÇ      (Optuna)         ‚îÇ
                        ‚îÇ                       ‚îÇ
                        ‚îÇ  ‚Ä¢ Study management   ‚îÇ
                        ‚îÇ  ‚Ä¢ Trial execution    ‚îÇ
                        ‚îÇ  ‚Ä¢ Pruning logic      ‚îÇ
                        ‚îÇ  ‚Ä¢ Visualization      ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Design Principles:**

1. Orchestrator owns both Config and InfrastructureManager
2. Config is the SSOT - all modules receive it as dependency
3. InfrastructureManager is stateless utility for OS-level operations
4. Execution pipeline is linear: Data ‚Üí Model ‚Üí Training ‚Üí Eval
5. Optimization wraps the entire pipeline for each trial

---

## üìä Experiment Management

Every run generates a complete artifact suite for total traceability:

**Artifact Structure:**
```
outputs/20260123_organcmnist_efficientnetb0_a3f7c2/
‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix_efficientnet_b0_224.png
‚îÇ   ‚îú‚îÄ‚îÄ training_curves_efficientnet_b0_224.png
‚îÇ   ‚îú‚îÄ‚îÄ sample_predictions_efficientnet_b0_224.png
‚îÇ   ‚îú‚îÄ‚îÄ param_importances.html          # Interactive importance plot (optimization)
‚îÇ   ‚îú‚îÄ‚îÄ optimization_history.html       # Trial progression (optimization)
‚îÇ   ‚îî‚îÄ‚îÄ parallel_coordinates.html       # Hyperparameter relationships (optimization)
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ training_summary.xlsx           # Comprehensive metrics spreadsheet
‚îÇ   ‚îú‚îÄ‚îÄ best_config.yaml                # Optimized configuration (optimization)
‚îÇ   ‚îú‚îÄ‚îÄ study_summary.json              # All trials metadata (optimization)
‚îÇ   ‚îî‚îÄ‚îÄ top_10_trials.xlsx              # Best configurations (optimization)
‚îú‚îÄ‚îÄ checkpoints/
‚îÇ   ‚îî‚îÄ‚îÄ best_efficientnetb0.pth         # Trained model weights
‚îî‚îÄ‚îÄ database/
    ‚îî‚îÄ‚îÄ study.db                        # SQLite storage for resumption (optimization)
```

> [!IMPORTANT]
> ### üìÇ [View Sample Artifacts](./docs/artifacts)
> Explore Excel reports, YAML configs, and diagnostic plots from real experiments.

---

## üß© Dependency Graph

<p align="center">
<img src="docs/framework_map.svg?v=4" width="900" alt="System Dependency Graph">
</p>

> *Generated via `pydeps`. Highlights the centralized Config hub and modular architecture.*

<details>
<summary>üõ†Ô∏è Regenerate Dependency Graph</summary>

```bash
pydeps orchard \
    --cluster \
    --max-bacon=0 \
    --max-module-depth=4 \
    --only orchard \
    --noshow \
    -T svg \
    -o docs/framework_map.svg
```

**Requirements:** `pydeps` + Graphviz (`sudo apt install graphviz` or `brew install graphviz`)

</details>

---

## üî¨ Technical Deep Dive

### Architecture Adaptation

**ResNet-18 for 28√ó28 Resolution**

Standard ResNet-18 is optimized for 224√ó224 ImageNet inputs. Direct application to 28√ó28 domains causes catastrophic information loss. Our adaptation strategy:

| Layer | Standard ResNet-18 | VisionForge Adapted | Rationale |
|-------|-------------------|---------------------|-----------|
| **Input Conv** | 7√ó7, stride=2, pad=3 | **3√ó3, stride=1, pad=1** | Preserve spatial resolution |
| **Max Pooling** | 3√ó3, stride=2 | **Identity (bypassed)** | Prevent 75% feature loss |
| **Stage 1 Input** | 56√ó56 (from 224) | **28√ó28 (from 28)** | Native resolution entry |

**Key Modifications:**
1. **Stem Redesign**: Replacing large-receptive-field convolution avoids immediate downsampling
2. **Pooling Removal**: MaxPool bypass maintains full spatial fidelity into residual stages
3. **Bicubic Weight Transfer**: Pretrained 7√ó7 weights are spatially interpolated to 3√ó3 geometry

---

**MiniCNN for 28√ó28 Resolution**

A compact, custom architecture designed specifically for low-resolution medical imaging:

| Component | Specification | Purpose |
|-----------|--------------|---------|
| **Architecture** | 3 conv blocks + global pooling | Fast convergence with minimal parameters |
| **Parameters** | ~50K | 220√ó fewer than ResNet-18-Adapted |
| **Input Processing** | 28√ó28 ‚Üí 14√ó14 ‚Üí 7√ó7 ‚Üí 1√ó1 | Progressive spatial compression |
| **Regularization** | Configurable dropout before FC | Overfitting prevention |

**Advantages:**
- **Speed**: 2-3 minutes for full 60-epoch training on GPU
- **Efficiency**: Ideal for rapid prototyping and ablation studies
- **Interpretability**: Simple architecture for educational purposes

---

**EfficientNet-B0 for 224√ó224 Resolution**

Implements compound scaling (depth, width, resolution) for optimal parameter efficiency:

| Feature | Specification | Benefit |
|---------|--------------|---------|
| **Architecture** | Mobile Inverted Bottleneck Convolution (MBConv) | Memory-efficient feature extraction |
| **Parameters** | ~5.3M | 50% fewer than ResNet-50 |
| **Pretrained Weights** | ImageNet-1k | Strong initialization for transfer learning |
| **Input Adaptation** | Dynamic first-layer modification for grayscale | Preserves pretrained knowledge via weight morphing |

---

**Vision Transformer Tiny (ViT-Tiny) for 224√ó224 Resolution**

Patch-based attention architecture with multiple pretrained weight variants:

| Feature | Specification | Benefit |
|---------|--------------|---------|
| **Architecture** | 12-layer transformer encoder | Global context modeling via self-attention |
| **Parameters** | ~5.7M | Comparable to EfficientNet-B0 |
| **Patch Size** | 16√ó16 (196 patches from 224√ó224) | Efficient sequence length for transformers |
| **Weight Variants** | ImageNet-1k, ImageNet-21k, ImageNet-21k‚Üí1k fine-tuned | Optuna-searchable pretraining strategies |

**Supported Weight Variants:**
1. `vit_tiny_patch16_224.augreg_in21k_ft_in1k`: ImageNet-21k pretrained, fine-tuned on 1k (recommended)
2. `vit_tiny_patch16_224.augreg_in21k`: ImageNet-21k pretrained (requires custom head tuning)
3. `vit_tiny_patch16_224`: ImageNet-1k baseline

---

### Mathematical Weight Transfer

To retain ImageNet-learned feature detectors when adapting to grayscale inputs, we apply bicubic interpolation for CNNs and channel averaging for ViT:

**CNN Weight Morphing (ResNet, EfficientNet):**

**Source Tensor:**
```math
W_{\text{src}} \in \mathbb{R}^{C_{\text{out}} \times C_{\text{in}} \times K \times K}
```

**Transformation:**
```math
W_{\text{dest}} = \mathcal{I}_{\text{bicubic}}(W_{\text{src}}, \text{size}=(K', K'))
```

For grayscale adaptation:
```math
W_{\text{gray}} = \frac{1}{3} \sum_{c=1}^{3} W_{\text{src}}[:, c, :, :]
```

**ViT Patch Embedding Adaptation:**
```math
W_{\text{gray}} = \text{mean}(W_{\text{src}}, \text{dim}=1) \quad \text{where} \quad W_{\text{src}} \in \mathbb{R}^{192 \times 3 \times 16 \times 16}
```

**Result:** Preserves learned edge detectors and texture patterns while adapting to custom input geometry.

---

### Training Regularization

**MixUp Augmentation** synthesizes training samples via convex combinations:

```math
\tilde{x} = \lambda x_i + (1 - \lambda) x_j \quad \text{where} \quad \lambda \sim \text{Beta}(\alpha, \alpha)
```

```math
\tilde{y} = \lambda y_i + (1 - \lambda) y_j
```

This prevents overfitting on small-scale textures and improves generalization.

---

## üìÅ Project Structure

```
visionforge/
‚îú‚îÄ‚îÄ main.py                         # Training entry point
‚îú‚îÄ‚îÄ optimize.py                     # Hyperparameter search entry point
‚îú‚îÄ‚îÄ Dockerfile                      # Multi-stage reproducible build
‚îú‚îÄ‚îÄ requirements.txt                # Pinned dependencies
‚îú‚îÄ‚îÄ recipes/                        # YAML configuration presets
‚îÇ   ‚îú‚îÄ‚îÄ config_resnet_18_adapted.yaml       # 28√ó28 transfer learning
‚îÇ   ‚îú‚îÄ‚îÄ config_mini_cnn.yaml                # 28√ó28 fast baseline
‚îÇ   ‚îú‚îÄ‚îÄ config_efficientnet_b0.yaml         # 224√ó224 efficient scaling
‚îÇ   ‚îú‚îÄ‚îÄ config_vit_tiny.yaml                # 224√ó224 transformer
‚îÇ   ‚îú‚îÄ‚îÄ optuna_resnet_18_adapted.yaml       # Optimization for 28√ó28
‚îÇ   ‚îú‚îÄ‚îÄ optuna_mini_cnn.yaml                # Architecture search 28√ó28
‚îÇ   ‚îú‚îÄ‚îÄ optuna_efficientnet_b0.yaml         # Optimization for 224√ó224
‚îÇ   ‚îî‚îÄ‚îÄ optuna_vit_tiny.yaml                # Weight variant search 224√ó224
‚îú‚îÄ‚îÄ tests/                          # Test suite (800+ tests, ‚Üí100% coverage)
‚îÇ   ‚îú‚îÄ‚îÄ smoke_test.py               # 1-epoch E2E verification (~30s)
‚îÇ   ‚îú‚îÄ‚îÄ health_check.py             # Dataset integrity validation
‚îÇ   ‚îú‚îÄ‚îÄ test_config/                # Config engine tests (210 tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_core/                  # Core utilities tests (95 tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_data_handler/          # Data loading tests (87 tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_evaluation/            # Metrics & viz tests (63 tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_models/                # Architecture tests (44 tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_optimization/          # Optuna integration tests (78 tests)
‚îÇ   ‚îî‚îÄ‚îÄ test_trainer/               # Training loop tests (142 tests)
‚îú‚îÄ‚îÄ orchard/                        # Core framework package
‚îÇ   ‚îú‚îÄ‚îÄ core/                       # Framework nucleus
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/                 # Pydantic schemas (8 modules)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ environment/            # Hardware abstraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ io/                     # Serialization utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger/                 # Telemetry system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata/               # Dataset registry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ paths/                  # Path management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cli.py                  # Argument parser
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py         # Lifecycle coordinator
‚îÇ   ‚îú‚îÄ‚îÄ data_handler/               # Loading strategies
‚îÇ   ‚îú‚îÄ‚îÄ models/                     # Architecture factory
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resnet_18_adapted.py    # Adapted ResNet for 28√ó28
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mini_cnn.py             # Compact CNN for 28√ó28
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ efficientnet_b0.py      # EfficientNet for 224√ó224
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vit_tiny.py             # Vision Transformer for 224√ó224
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ factory.py              # Architecture registry
‚îÇ   ‚îú‚îÄ‚îÄ trainer/                    # Training loop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engine.py               # Core training/validation logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py              # ModelTrainer orchestrator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ losses.py               # Custom loss functions (FocalLoss)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ setup.py                # Optimizer/scheduler setup
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/                 # Metrics and visualization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engine.py               # Evaluation orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py              # AUC, F1, Accuracy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visualization.py        # Plots (confusion matrix, curves)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reporting.py            # Excel report generation
‚îÇ   ‚îî‚îÄ‚îÄ optimization/               # Optuna integration
‚îÇ       ‚îú‚îÄ‚îÄ objective/              # Trial execution logic
‚îÇ       ‚îú‚îÄ‚îÄ orchestrator/           # Study management & visualization
‚îÇ       ‚îú‚îÄ‚îÄ search_spaces.py        # Hyperparameter distributions
‚îÇ       ‚îî‚îÄ‚îÄ early_stopping.py       # Convergence detection
‚îî‚îÄ‚îÄ outputs/                        # Isolated run workspaces
    ‚îî‚îÄ‚îÄ YYYYMMDD_dataset_model_hash/
        ‚îú‚îÄ‚îÄ figures/                # Visualizations (PNG, HTML)
        ‚îú‚îÄ‚îÄ reports/                # Metrics (XLSX, JSON, YAML)
        ‚îú‚îÄ‚îÄ models/                 # Model weights (.pth)
        ‚îî‚îÄ‚îÄ database/               # Optuna study (SQLite)
```

---

## üíª Usage Patterns

### Configuration-Driven Execution

**Recommended Method:** YAML recipes ensure full reproducibility and version control.

```bash
# Verify environment (~30 seconds)
python -m tests.smoke_test

# Train with presets (28√ó28 resolution, CPU-compatible)
python main.py --config recipes/config_resnet_18_adapted.yaml     # ~5 min GPU, ~2.5h CPU
python main.py --config recipes/config_mini_cnn.yaml              # ~2-3 min GPU, ~30 min CPU

# Train with presets (224√ó224 resolution, GPU required)
python main.py --config recipes/config_efficientnet_b0.yaml       # ~30 min per trial
python main.py --config recipes/config_vit_tiny.yaml              # ~25-35 min per trial
```

### CLI Overrides

For rapid experimentation (not recommended for production):

```bash
# Quick test on different dataset
python main.py --dataset dermamnist --epochs 10 --batch_size 64

# Custom learning rate schedule
python main.py --lr 0.001 --min_lr 1e-7 --epochs 100

# Disable augmentations
python main.py --mixup_alpha 0 --no_tta
```

> [!WARNING]
> **Configuration Precedence Order:**
> 1. **YAML file** (highest priority - if `--config` is provided)
> 2. **CLI arguments** (only used when no `--config` specified)
> 3. **Defaults** (from Pydantic field definitions)
>
> **When `--config` is provided, YAML values override CLI arguments.** This prevents configuration drift but means CLI flags are ignored. For reproducible research, always use YAML recipes.

---

## üéØ Hyperparameter Optimization

### Quick Start

```bash
# Install Optuna (if not already present)
pip install optuna plotly timm  # timm required for ViT support

# Run optimization with presets
python optimize.py --config recipes/optuna_resnet_18_adapted.yaml  # 50 trials, ~3-5h
python optimize.py --config recipes/optuna_mini_cnn.yaml           # 50 trials, ~1-2h

# 224√ó224 resolution (includes weight variant search for ViT)
python optimize.py --config recipes/optuna_efficientnet_b0.yaml    # 20 trials, ~1.5-5h
python optimize.py --config recipes/optuna_vit_tiny.yaml           # 20 trials, ~3-5h

# Custom search (20 trials, 10 epochs each)
python optimize.py --dataset pathmnist \
    --n_trials 20 \
    --epochs 10 \
    --search_space_preset quick

# Resume interrupted study
python optimize.py --config recipes/optuna_vit_tiny.yaml \
    --load_if_exists true
```

### Search Space Coverage

**Full Space** (13+ parameters):
- **Optimization**: `learning_rate`, `weight_decay`, `momentum`, `min_lr`
- **Regularization**: `mixup_alpha`, `label_smoothing`, `dropout`
- **Scheduling**: `cosine_fraction`, `scheduler_patience`
- **Augmentation**: `rotation_angle`, `jitter_val`, `min_scale`
- **Batch Size**: Resolution-aware categorical choices
  - 28√ó28: [16, 32, 48, 64]
  - 224√ó224: [8, 12, 16] (OOM-safe for 8GB VRAM)
- **Architecture** (resolution-specific):
  - 28√ó28: [`resnet_18_adapted`, `mini_cnn`]
  - 224√ó224: [`efficientnet_b0`, `vit_tiny`]
- **Weight Variants** (ViT only, 224√ó224):
  - `vit_tiny_patch16_224.augreg_in21k_ft_in1k`
  - `vit_tiny_patch16_224.augreg_in21k`
  - Default variant

**Quick Space** (4 parameters):
- `learning_rate`, `weight_decay`, `batch_size`, `dropout`

### Optimization Workflow

```bash
# Phase 1: Comprehensive search (configurable trials, early stopping enabled)
python optimize.py --config recipes/optuna_efficientnet_b0.yaml

# Phase 2: Review results
firefox outputs/*/figures/param_importances.html
firefox outputs/*/figures/optimization_history.html

# Phase 3: Train with best config (60 epochs, full evaluation)
python main.py --config outputs/*/reports/best_config.yaml
```

### Artifacts Generated

```
outputs/20260123_organcmnist_efficientnetb0_a3f7c2/
‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îú‚îÄ‚îÄ param_importances.html      # Interactive importance plot
‚îÇ   ‚îú‚îÄ‚îÄ optimization_history.html   # Trial progression
‚îÇ   ‚îú‚îÄ‚îÄ slice.html                  # 1D parameter effects
‚îÇ   ‚îî‚îÄ‚îÄ parallel_coordinate.html    # Multi-dimensional view
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ best_config.yaml            # Optimized configuration
‚îÇ   ‚îú‚îÄ‚îÄ study_summary.json          # All trials metadata
‚îÇ   ‚îî‚îÄ‚îÄ top_10_trials.xlsx          # Best configurations
‚îî‚îÄ‚îÄ database/
    ‚îî‚îÄ‚îÄ study.db                    # SQLite storage for resumption
```

### Customization

Edit search spaces in `orchard/optimization/search_spaces.py`:

```python
class CustomSearchSpace:
    @staticmethod
    def get_optimization_space() -> Dict[str, Callable]:
        return {
            "learning_rate": lambda trial: trial.suggest_float(
                "learning_rate", 1e-4, 1e-2, log=True
            ),
            "weight_decay": lambda trial: trial.suggest_float(
                "weight_decay", 1e-5, 1e-3, log=True
            ),
        }
```

---

## ‚úÖ Environment Verification

**Smoke Test** (1-epoch sanity check):
```bash
# Default: BloodMNIST 28√ó28
python -m tests.smoke_test

# Custom dataset/resolution
python -m tests.smoke_test --dataset pathmnist --resolution 224
```

**Output:** Validates full pipeline in <30 seconds:
- Dataset loading and preprocessing
- Model instantiation and weight transfer
- Training loop execution
- Evaluation metrics computation
- Excel/PNG artifact generation

**Health Check** (dataset integrity):
```bash
python -m tests.health_check --dataset bloodmnist
```

**Output:** Verifies:
- MD5 checksum matching
- NPZ key structure (`train_images`, `train_labels`, `val_images`, etc.)
- Sample count validation

---

## üê≥ Containerized Deployment

### Build Image

```bash
docker build -t visionforge:latest .
```

### Execution Modes

**Standard Mode** (Performance Optimized):
```bash
docker run -it --rm \
  --gpus all \
  -u $(id -u):$(id -g) \
  -e TORCH_HOME=/tmp/torch_cache \
  -e MPLCONFIGDIR=/tmp/matplotlib_cache \
  -v $(pwd)/dataset:/app/dataset \
  -v $(pwd)/outputs:/app/outputs \
  visionforge:latest \
  --config recipes/config_resnet_18_adapted.yaml
```

**Strict Reproducibility Mode** (Bit-Perfect Determinism):
```bash
docker run -it --rm \
  --gpus all \
  -u $(id -u):$(id -g) \
  -e IN_DOCKER=TRUE \
  -e DOCKER_REPRODUCIBILITY_MODE=TRUE \
  -e TORCH_HOME=/tmp/torch_cache \
  -e MPLCONFIGDIR=/tmp/matplotlib_cache \
  -e PYTHONHASHSEED=42 \
  -e CUBLAS_WORKSPACE_CONFIG=:4096:8 \
  -v $(pwd)/dataset:/app/dataset \
  -v $(pwd)/outputs:/app/outputs \
  visionforge:latest \
  --config recipes/config_resnet_18_adapted.yaml
```

> [!NOTE]
> - `TORCH_HOME` and `MPLCONFIGDIR` prevent permission errors in containerized environments
> - `CUBLAS_WORKSPACE_CONFIG` is required for CUDA determinism
> - `--gpus all` requires NVIDIA Container Toolkit

---

## üìä Configuration Reference

### Core Parameters

| Parameter | Type | Default | Range | Description |
|-----------|------|---------|-------|-------------|
| `epochs` | int | 60 | [1, 1000] | Training epochs |
| `batch_size` | int | 128 | [1, 2048] | Samples per batch |
| `learning_rate` | float | 0.008 | (1e-8, 1.0) | Initial SGD learning rate |
| `min_lr` | float | 1e-6 | (0, lr) | Minimum LR for scheduler |
| `weight_decay` | float | 5e-4 | [0, 0.2] | L2 regularization |
| `momentum` | float | 0.9 | [0, 1] | SGD momentum |
| `mixup_alpha` | float | 0.002 | [0, 1] | MixUp strength (0=disabled) |
| `label_smoothing` | float | 0.0 | [0, 0.3] | Label smoothing factor |
| `dropout` | float | 0.0 | [0, 0.9] | Dropout probability |
| `seed` | int | 42 | - | Global random seed |
| `reproducible` | bool | False | - | Enable strict determinism |

### Augmentation Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `hflip` | float | 0.5 | Horizontal flip probability |
| `rotation_angle` | int | 10 | Max rotation degrees |
| `jitter_val` | float | 0.2 | ColorJitter intensity |
| `min_scale` | float | 0.95 | Minimum RandomResizedCrop scale |
| `no_tta` | bool | False | Disable test-time augmentation |

### Model Parameters

| Parameter | Type | Default | Options |
|-----------|------|---------|---------|
| `model_name` | str | "resnet_18_adapted" | `resnet_18_adapted`, `mini_cnn` (28√ó28); `efficientnet_b0`, `vit_tiny` (224√ó224) |
| `pretrained` | bool | True | Use ImageNet weights (N/A for MiniCNN) |
| `weight_variant` | str | None | ViT-specific pretrained variant (e.g., `augreg_in21k_ft_in1k`) |
| `force_rgb` | bool | True | Convert grayscale to 3-channel |
| `resolution` | int | 28 | [28, 224] |

### Dataset Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `dataset` | str | "bloodmnist" | MedMNIST identifier |
| `data_root` | Path | `./dataset` | Dataset directory |
| `max_samples` | int | None | Cap training samples (debugging) |
| `use_weighted_sampler` | bool | True | Balance class distribution |

---

## üîÑ Extending to New Datasets

The framework is designed for zero-code dataset integration via the registry system:

### 1. Add Dataset Metadata

Edit `orchard/core/metadata/medmnist_v2_28x28.py` or `medmnist_v2_224x224.py`:

```python
DATASET_REGISTRY = {
    "custom_dataset": DatasetMetadata(
        name="custom_dataset",
        num_classes=10,
        in_channels=3,
        mean=(0.5, 0.5, 0.5),
        std=(0.25, 0.25, 0.25),
        native_resolution=28,
        class_names=["class0", "class1", ...],
        url="https://example.com/dataset.npz",
        md5="abc123...",
        is_anatomical=False,
        is_texture_based=True
    ),
}
```

### 2. Train Immediately

```bash
python main.py --dataset custom_dataset --epochs 30
```

No code changes required‚Äîthe configuration engine automatically resolves metadata.

---

## üß™ Testing & Quality Assurance

### Test Suite

VisionForge includes a comprehensive test suite with **800+ tests** targeting **‚Üí100% code coverage**:

```bash
# Run full test suite
pytest tests/ -v

# Run with coverage report
pytest tests/ --cov=orchard --cov-report=html

# Run specific test categories
pytest tests/ -m unit          # Unit tests only
pytest tests/ -m integration   # Integration tests only

# Run parallel tests (faster)
pytest tests/ -n auto
```

### Test Categories

- **Unit Tests** (650+ tests): Config validation, metadata injection, type safety
- **Integration Tests** (150+ tests): End-to-end pipeline validation, YAML hydration
- **Smoke Tests**: 1-epoch sanity checks (~30 seconds)
- **Health Checks**: Dataset integrity validation


### Continuous Integration

GitHub Actions automatically run on every push:

- ‚úÖ Code quality checks (Black, isort, Flake8)
- ‚úÖ Unit tests across Python 3.10, 3.11, 3.12
- ‚úÖ Smoke tests (E2E validation)
- ‚úÖ Dataset health checks
- ‚úÖ Security scanning (Bandit, Safety)
- ‚úÖ Code coverage reporting (Codecov)

View the latest build status: [![CI/CD](https://github.com/tomrussobuilds/visionforge/actions/workflows/ci.yml/badge.svg)](https://github.com/tomrussobuilds/visionforge/actions/workflows/ci.yml)

---

## üìö Citation

```bibtex
@software{visionforge2025,
  author = {Tommaso Russo},
  title  = {VisionForge: Type-Safe Deep Learning Framework},
  year   = {2025},
  url    = {https://github.com/tomrussobuilds/visionforge},
  note   = {PyTorch framework with Pydantic configuration and Optuna optimization}
}
```

---

## üó∫ Development Roadmap

### ‚úÖ Phase 1: Foundation (Completed)
- Architecture adaptation (3√ó3 stem, MaxPool removal)
- Pydantic-based configuration engine
- Infrastructure safety (flock, process management)

### ‚úÖ Phase 2: Automation (Completed)
- YAML-driven execution model
- Optuna hyperparameter optimization
- Multi-resolution support (28√ó28, 224√ó224)
- Comprehensive test suite (800+ tests)
- CI/CD pipeline with GitHub Actions

### ‚úÖ Phase 3: Modern Architectures (Completed)
- **Vision Transformer (ViT-Tiny)**: Patch-based attention with 3 weight variants
- **MiniCNN**: Compact baseline for rapid prototyping (~50K parameters)
- **Architecture Search**: Optuna-driven model selection for both resolutions
- **Weight Variant Search**: Automatic exploration of ImageNet-1k/21k pretraining strategies
- **ONNX Export**: Model serialization for deployment (via `onnx` extra)

### üîÆ Phase 4: Advanced Features (In Progress)
- **Additional Architectures**: ConvNeXt, EfficientNet-V2, DeiT
- **Domain Extension**: Abstract dataset registry for non-medical domains
- **Multi-modal Support**: Detection, segmentation hooks
- **Distributed Training**: DDP, FSDP support for multi-GPU
- **Advanced Export**: TorchScript optimization, quantization, ONNX Runtime
- **Benchmark Suite**: Standardized architecture comparison framework

### üéØ Current Status
- **Test Coverage**: 98% (‚Üí100% goal, 800+ tests)
- **Architectures**: 4 total (2 for 28√ó28, 2 for 224√ó224)
  - 28√ó28: ResNet-18-Adapted, MiniCNN
  - 224√ó224: EfficientNet-B0, ViT-Tiny
- **Resolutions**: 2 (28√ó28, 224√ó224)
- **Export Formats**: PyTorch (.pth), ONNX (.onnx with optional dependency)

---

## üìÑ License

MIT License - See [LICENSE](LICENSE) for details.

## ü§ù Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass: `pytest tests/ -v`
5. Submit a pull request

## üìß Contact

For questions or collaboration: [GitHub Issues](https://github.com/tomrussobuilds/visionforge/issues)

---

<p align="center">
<strong>Built with ‚ù§Ô∏è for reproducible research</strong>
</p>